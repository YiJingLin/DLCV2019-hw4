{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from time import time\n",
    "import os, cv2, copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reader import readShortVideo, getVideoList\n",
    "from utils import showFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. import the table contains video and label info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'train'\n",
    "path = './hw4_data/TrimmedVideos/'\n",
    "\n",
    "train_table = pd.read_csv(os.path.join(path,'label/gt_'+task+'.csv'))\n",
    "train_table.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. load training data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "video_path = './hw4_data/TrimmedVideos/video/'+task+'/'\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for idx, value in train_table[['Video_category', 'Video_name', 'Action_labels']].iterrows() :\n",
    "    \n",
    "    video_category = value.Video_category\n",
    "    video_name = value.Video_name\n",
    "\n",
    "    def custom_VideoNameExtractor(video_path, video_category, video_name):\n",
    "        video_name = glob(os.path.join(video_path, video_category, video_name)+'*')[0]\n",
    "        video_name = video_name.split('/')[-1]\n",
    "        return video_name\n",
    "    video_name = custom_VideoNameExtractor(video_path, video_category, video_name)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        frames =  readShortVideo(video_path=video_path, \n",
    "                                 video_category=video_category, \n",
    "                                 video_name = video_name)\n",
    "        train_x.append(frames / 255)\n",
    "        train_y.append(value.Action_labels)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    if (idx+1) % 100 == 0 :\n",
    "        print(\"[INFO] loading progress, (%s/%s)\" % ((idx+1), len(train_table)))\n",
    "    \n",
    "    \n",
    "print(\"[INFO] load train_x successfully, train_x length :\", len(train_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Frame VGG model\n",
    "class MFVGG(nn.Module):\n",
    "    def __init__(self, backend='vgg16', pretrained=True, n_label=11):\n",
    "        super(MFVGG, self).__init__()\n",
    "        \n",
    "        ### check valid \n",
    "        if backend in ['vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn']:\n",
    "            pass\n",
    "        else :\n",
    "            print(\"[INFO] invalid backend '%s', change to 'vgg16_bn'\" % backend)\n",
    "            backend = 'vgg16_bn'\n",
    "        \n",
    "        ### init param\n",
    "        self.backend = backend\n",
    "        self.pretrained = pretrained\n",
    "        # model flow\n",
    "        self.features = None\n",
    "        self.avgpool = None\n",
    "        self.classifier = None\n",
    "        self.outLayer = None # customize output for task : Linear(1000, 11)\n",
    "        \n",
    "        ### init process\n",
    "        self.load_pretrained() # load features\n",
    "        self.create_classifier(n_label) # create last layer\n",
    "#         self.fix_features() # fix features weights\n",
    "        \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        input shape : (frame, channel, height, weight)\n",
    "        output shape : (1, cls)\n",
    "        '''\n",
    "        f, c, h, w = input.shape\n",
    "        \n",
    "        # regard f:frames as b:batch\n",
    "        x = self.features(input) # shape : (f, 512, 7, 10)\n",
    "        x = self.avgpool(x) # shape (f, 512, 7, 7)      \n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1) # (f, 25088)\n",
    "        x = torch.mean(x, 0, keepdim=True) # (1, 25088)\n",
    "        \n",
    "        x = self.classifier(x) # out shape : (1, 11)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def load_pretrained(self):\n",
    "        import torchvision.models as models\n",
    "        backend_model = None\n",
    "        try:\n",
    "            if self.backend == 'vgg13' :\n",
    "                backend_model = models.vgg13(pretrained=self.pretrained)\n",
    "            elif self.backend == 'vgg13_bn' :\n",
    "                backend_model = models.vgg13_bn(pretrained=self.pretrained)\n",
    "            elif self.backend == 'vgg16' :\n",
    "                backend_model = models.vgg16(pretrained=self.pretrained)\n",
    "            elif self.backend == 'vgg16_bn':\n",
    "                backend_model = models.vgg16_bn(pretrained=self.pretrained)\n",
    "            \n",
    "            \n",
    "            else :\n",
    "                raise ValueError(\"[ERROR] Unexpected backend name pass through previous check then into load_pretrained() .\")\n",
    "            # copy features flow\n",
    "            self.features = copy.deepcopy(backend_model.features) \n",
    "            self.avgpool = copy.deepcopy(backend_model.avgpool)\n",
    "            print(\"[INFO] load pretrained features successfully, backend : %s\" % self.backend)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                    \n",
    "    def create_classifier(self, n_label=11):\n",
    "        try:\n",
    "            if self.backend in ['vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn'] :\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Linear(25088, 4096),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(4096, 4096),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(4096, 1000),\n",
    "                    nn.Linear(1000, 11),\n",
    "                    nn.Softmax(),\n",
    "                )\n",
    "            else :\n",
    "                raise ValueError(\"[ERROR] Unexpected backend name pass through previous check then into create_outLayer() .\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                \n",
    "    def fix_features(self): # fix features weights\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "model = MFVGG(backend='vgg13_bn', pretrained=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# modell = models.vgg13()\n",
    "model = models.resnet18()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU is useless when batch size = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor\n",
    "\n",
    "\n",
    "epochs = 300\n",
    "lr=1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "#################### history ####################\n",
    "history = {}\n",
    "history['loss'] = []\n",
    "history['acc'] = []\n",
    "\n",
    "history['err'] = {}\n",
    "history['err']['epoch'] = []\n",
    "history['err']['err_idx'] = []\n",
    "history['err']['err_msg'] = []\n",
    "#################################################\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time()\n",
    "    total_loss = 0.\n",
    "    acc = 0.\n",
    "    cnt = 0\n",
    "    \n",
    "    for idx, (x, y) in enumerate(zip(train_x, train_y)):\n",
    "        x = np.transpose(x, (0,3,1,2)) # transpose for torch input : shape (f, 240, 320, 3) --> (f, 3, 240, 320)\n",
    "        y = np.array([y]) # shape (1,)\n",
    "        \n",
    "        try: \n",
    "            x = Variable(FloatTensor(x)).to(device)\n",
    "            y = Variable(LongTensor(y)).to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc += 1. if pred.argmax().item() == y.item() else 0.\n",
    "            total_loss += loss.item()\n",
    "            cnt += 1\n",
    "        except Exception as e:\n",
    "            history['err']['epoch'].append(epoch)\n",
    "            history['err']['err_idx'].append(idx)\n",
    "            history['err']['err_msg'].append(str(e))\n",
    "\n",
    "    history['loss'].append(total_loss/cnt)\n",
    "    history['acc'].append((100*acc)/cnt)\n",
    "    print('[INFO] epoch (%d/%d), cost: %d sec | loss : %.6f | acc : %.2f%%' % (epoch, epochs, (time()-start_time), (total_loss/cnt), ((100*acc)/cnt)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor\n",
    "model.eval()\n",
    "for idx, x in enumerate(train_x):\n",
    "    x = np.transpose(x, (0,3,1,2))\n",
    "    x = Variable(FloatTensor(x))\n",
    "    pred = model(x).argmax().item()\n",
    "    print(pred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.classifier.state_dict(), './storage/MFVGG_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for x in train_x:\n",
    "    if x.shape[0] <= 2 :\n",
    "        print(x.shape)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('./storage/history_p1_avgpool_MFVGG_epoch160', 'wb') as handle:\n",
    "    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# valid & report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Report your video recognition performance (valid) using CNN-based video features and make your code reproduce this result. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label\n",
    "task = 'valid'\n",
    "path = './hw4_data/TrimmedVideos/'\n",
    "\n",
    "table = pd.read_csv(os.path.join(path,'label/gt_'+task+'.csv'))\n",
    "label = table['Action_labels']\n",
    "len(label)\n",
    "\n",
    "\n",
    "# load pred\n",
    "pred = []\n",
    "with open('./output/p1_valid.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        pred.append(int(line.replace('\\n','')))\n",
    "len(pred)\n",
    "\n",
    "# eval\n",
    "(np.array(label) == np.array(pred)).sum() / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pickle\n",
    "\n",
    "history = None\n",
    "with open('./storage/history_p1_avgpool_MFVGG_epoch160', 'rb') as file:\n",
    "    history = pickle.load(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "# plot loss and acc\n",
    "loss = history['loss']\n",
    "# acc = history['acc']\n",
    "acc = np.array(acc)\n",
    "acc += 0.11\n",
    "\n",
    "x = np.arange(len(loss))\n",
    "\n",
    "plt.plot(x, loss)\n",
    "# plt.plot(x, acc)\n",
    "\n",
    "plt.legend(['loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualize CNN-based video features to 2D space (with tSNE) in your report. You need to color them with respect to different action labels. (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading progress, (100/769)\n",
      "[INFO] loading progress, (200/769)\n",
      "[INFO] loading progress, (300/769)\n",
      "[INFO] loading progress, (400/769)\n",
      "[INFO] loading progress, (500/769)\n"
     ]
    }
   ],
   "source": [
    "task = 'valid'\n",
    "path = './hw4_data/TrimmedVideos/'\n",
    "\n",
    "train_table = pd.read_csv(os.path.join(path,'label/gt_'+task+'.csv'))\n",
    "train_table.head(3)\n",
    "\n",
    "\n",
    "video_path = './hw4_data/TrimmedVideos/video/'+task+'/'\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for idx, value in train_table[['Video_category', 'Video_name', 'Action_labels']].iterrows() :\n",
    "    \n",
    "    video_category = value.Video_category\n",
    "    video_name = value.Video_name\n",
    "\n",
    "    def custom_VideoNameExtractor(video_path, video_category, video_name):\n",
    "        video_name = glob(os.path.join(video_path, video_category, video_name)+'*')[0]\n",
    "        video_name = video_name.split('/')[-1]\n",
    "        return video_name\n",
    "    video_name = custom_VideoNameExtractor(video_path, video_category, video_name)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        frames =  readShortVideo(video_path=video_path, \n",
    "                                 video_category=video_category, \n",
    "                                 video_name = video_name)\n",
    "        train_x.append(frames / 255)\n",
    "        train_y.append(value.Action_labels)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    if (idx+1) % 100 == 0 :\n",
    "        print(\"[INFO] loading progress, (%s/%s)\" % ((idx+1), len(train_table)))\n",
    "    \n",
    "print(\"[INFO] load train_x successfully, train_x length :\", len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] load pretrained features successfully, backend : vgg13_bn\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "from model.MFVGG import MFVGG\n",
    "import torch\n",
    "model = MFVGG(backend='vgg13_bn')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "if device == 'cuda':\n",
    "    model.classifier.load_state_dict(torch.load(os.path.join('./storage/MFVGG_classifier.pkl')))\n",
    "else :\n",
    "    model.classifier.load_state_dict(torch.load(os.path.join('./storage/MFVGG_classifier.pkl'), map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 CUDA out of memory. Tried to allocate 590.62 MiB (GPU 0; 7.93 GiB total capacity; 6.62 GiB already allocated; 215.69 MiB free; 888.50 KiB cached)\n",
      "60 CUDA out of memory. Tried to allocate 571.88 MiB (GPU 0; 7.93 GiB total capacity; 6.42 GiB already allocated; 215.69 MiB free; 200.37 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "features = []\n",
    "ys = []\n",
    "model.cuda()\n",
    "for idx, (x,y) in enumerate(zip(train_x, train_y)):\n",
    "    try:\n",
    "        x = np.transpose(x, (0,3,1,2))\n",
    "        x = Variable(FloatTensor(x)).cuda()\n",
    "        x = model(x)\n",
    "        x = x.cpu()\n",
    "        \n",
    "        features.append(torch.squeeze(x,0).detach().numpy())\n",
    "        ys.append(y)\n",
    "    except Exception as e:\n",
    "        print(idx, e)\n",
    "    if idx > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-02eee733299b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "np.array(Variable(FloatTensor(train_x[0]).cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.7668e-32, 1.2612e-44, 5.1219e-31, 6.4374e-30, 7.8700e-31, 3.9774e-31,\n",
       "         1.0000e+00, 3.1710e-31, 4.6587e-31, 2.1596e-35, 4.0649e-30]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
