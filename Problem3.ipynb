{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from time import time\n",
    "import os, cv2, copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reader import readShortVideo, getVideoList\n",
    "from utils import showFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. import the table contains video and label info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] load images and label for video : OP04-R05-Cheeseburger ...finish\n",
      "[INFO] load images and label for video : OP02-R05-Cheeseburger ...finish\n",
      "[INFO] load images and label for video : OP04-R03-BaconAndEggs ...finish\n",
      "[INFO] load images and label for video : OP05-R03-BaconAndEggs ...finish\n",
      "[INFO] load images and label for video : OP06-R07-Pizza ...finish\n",
      "[INFO] load images and label for video : OP03-R01-PastaSalad ...finish\n",
      "[INFO] load images and label for video : OP03-R07-Pizza ...finish\n",
      "[INFO] load images and label for video : OP01-R01-PastaSalad ...finish\n",
      "[INFO] load images and label for video : OP04-R07-Pizza ...finish\n",
      "[INFO] load images and label for video : OP01-R06-GreekSalad ...finish\n",
      "[INFO] load images and label for video : OP01-R05-Cheeseburger ...finish\n",
      "[INFO] load images and label for video : OP05-R07-Pizza ...finish\n",
      "[INFO] load images and label for video : OP02-R03-BaconAndEggs ...finish\n",
      "[INFO] load images and label for video : OP02-R07-Pizza ...finish\n",
      "[INFO] load images and label for video : OP03-R02-TurkeySandwich ...finish\n",
      "[INFO] load images and label for video : OP04-R06-GreekSalad ...finish\n",
      "[INFO] load images and label for video : OP01-R03-BaconAndEggs ...finish\n",
      "[INFO] load images and label for video : OP06-R06-GreekSalad ...finish\n",
      "[INFO] load images and label for video : OP02-R06-GreekSalad ...finish\n",
      "[INFO] load images and label for video : OP02-R01-PastaSalad ...finish\n",
      "[INFO] load images and label for video : OP04-R01-PastaSalad ...finish\n",
      "[INFO] load images and label for video : OP06-R04-ContinentalBreakfast ...finish\n",
      "[INFO] load images and label for video : OP02-R04-ContinentalBreakfast ...finish\n"
     ]
    }
   ],
   "source": [
    "task = \"train\"\n",
    "afterCnt = 300 # we want to down-sample to 300 images for each videos\n",
    "\n",
    "names = []\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "for label_path in glob('./hw4_data/FullLengthVideos/labels/'+task+'/*') :\n",
    "    name = label_path.split('/')[-1].replace('.txt','')\n",
    "    print('[INFO] load images and label for video :', name, '...', end='')\n",
    "    \n",
    "    # load images paths\n",
    "    img_paths = glob(os.path.join('./hw4_data/FullLengthVideos/videos/'+task, name, '*'))\n",
    "    img_paths.sort()\n",
    "    \n",
    "    # load labels\n",
    "    labels = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for label in file:\n",
    "            labels.append(int(label.replace('\\n','')))\n",
    "    \n",
    "    # check if lengths of img_paths and labels are match\n",
    "    if not len(img_paths) == len(labels) :\n",
    "        raise ValueError('[ERROR] Mismatch of length between frames and labels.')\n",
    "    \n",
    "    # down-sample\n",
    "    idx_lst = []\n",
    "    rawCnt = len(img_paths)\n",
    "    interval = rawCnt / afterCnt\n",
    "    for idx in range(1, afterCnt+1):\n",
    "        idx_lst.append(int(interval * idx) - 1) # select index for down-sample step\n",
    "    \n",
    "    # select down-sample labels and image paths\n",
    "    labels = np.array(labels)\n",
    "    img_paths = np.array(img_paths)\n",
    "    \n",
    "    labels = labels[idx_lst]\n",
    "    img_paths = img_paths[idx_lst]\n",
    "    \n",
    "    # load images\n",
    "    images = []\n",
    "    for img_path in img_paths:\n",
    "        images.append(cv2.imread(img_path))\n",
    "    images = np.array(images)\n",
    "    images = images[:,:,:,::-1]\n",
    "    \n",
    "    # finally put into train_x and train_y\n",
    "    names.append(name)\n",
    "    train_x.append(images)\n",
    "    train_y.append(labels)\n",
    "    \n",
    "    print('finish') \n",
    "    \n",
    "    \n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] load pretrained features successfully, backend : vgg13_bn\n",
      "[INFO] create RNN component successfully, rnn : GRU .\n",
      "[INFO] create classifier successfully.\n",
      "[INFO] load Problem2 pretrained weight successfully.\n"
     ]
    }
   ],
   "source": [
    "# Full Video VGG model\n",
    "class FVrnnVGG(nn.Module):\n",
    "    def __init__(self, backend='vgg16', pretrained=True, n_label=11):\n",
    "        super(FVrnnVGG, self).__init__()\n",
    "        \n",
    "        ### check valid \n",
    "        if backend in ['vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn']:\n",
    "            pass\n",
    "        else :\n",
    "            print(\"[INFO] invalid backend '%s', change to 'vgg16_bn'\" % backend)\n",
    "            backend = 'vgg16_bn'\n",
    "            \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        ### init param\n",
    "        self.backend = backend\n",
    "        self.pretrained = pretrained\n",
    "        # model flow\n",
    "        self.features = None\n",
    "        self.avgpool = None\n",
    "        self.RNN = None\n",
    "        self.h0 = None # follow RNN\n",
    "        self.c0 = None # follow RNN\n",
    "        self.classifier = None\n",
    "        \n",
    "        ### init process\n",
    "        self.load_pretrained() # load features\n",
    "        self.create_RNN() # create RNN \n",
    "        self.create_classifier(n_label) # create last layer\n",
    "        self.fix_features() # fix features weights\n",
    "        self.load_Problem2_pretrain()\n",
    "        \n",
    "    def forward(self, input, h=None):\n",
    "        '''\n",
    "        input shape : (frame, channel, height, weight)\n",
    "        output shape : (1, cls)\n",
    "        '''\n",
    "        \n",
    "        # regard f:frames as b:batch\n",
    "        x = self.features(input) # shape : (f, 512, 7, 10)\n",
    "        x = self.avgpool(x) # shape (f, 512, 7, 7)      \n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1) # (f, 25088)\n",
    "        x = torch.unsqueeze(x,0) # (1, f, 25088)\n",
    "        \n",
    "        if type(h) == None :\n",
    "            h = self.h0\n",
    "        out, h = self.RNN(x, h) # out(1, f, 1024) & (num_layers=1, 1, 1024)\n",
    "        x = torch.squeeze(out, 0) # (f, 1024)        \n",
    "        \n",
    "        x = self.classifier(x) # out shape : (f, 11)\n",
    "        return x, h # (f, 11) & (1, 1, 1024)\n",
    "    \n",
    "    def load_pretrained(self):\n",
    "        import torchvision.models as models\n",
    "        backend_model = None\n",
    "        try:\n",
    "            if self.backend == 'vgg13' :\n",
    "                backend_model = models.vgg13(pretrained=self.pretrained)\n",
    "            elif self.backend == 'vgg13_bn' :\n",
    "                backend_model = models.vgg13_bn(pretrained=self.pretrained)\n",
    "            elif self.backend == 'vgg16' :\n",
    "                backend_model = models.vgg16(pretrained=self.pretrained)\n",
    "            elif self.backend == 'vgg16_bn':\n",
    "                backend_model = models.vgg16_bn(pretrained=self.pretrained)\n",
    "            \n",
    "            \n",
    "            else :\n",
    "                raise ValueError(\"[ERROR] Unexpected backend name pass through previous check then into load_pretrained() .\")\n",
    "            # copy features flow\n",
    "            self.features = copy.deepcopy(backend_model.features) \n",
    "            self.avgpool = copy.deepcopy(backend_model.avgpool)\n",
    "            print(\"[INFO] load pretrained features successfully, backend : %s\" % self.backend)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def create_RNN(self, rnn='GRU', hidden_size=1024, num_layers=1, batch_first=True):\n",
    "        '''\n",
    "        output (batch, seq, hidden_size)\n",
    "        h_out (n_layer, batch, hidden_size)\n",
    "        '''\n",
    "        try:\n",
    "            input_size = None\n",
    "            if self.backend in ['vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn']:\n",
    "                input_size = 25088\n",
    "            else :\n",
    "                raise ValueError(\"[ERROR] Unexpected backend name pass through previous check then into create_outLayer() .\")\n",
    "            \n",
    "            if rnn == 'GRU' :\n",
    "                self.RNN = nn.GRU(\n",
    "                    input_size=input_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=num_layers, \n",
    "                    batch_first=batch_first,\n",
    "                )\n",
    "                self.h0 = Variable(torch.zeros((num_layers,1,hidden_size)), requires_grad=False).to(self.device) # bach_size = 1\n",
    "            \n",
    "            else :\n",
    "                raise ValueError(\"[ERROR] Unexpected rnn '%s', please select one in ['GRU']\" & rnn)\n",
    "                \n",
    "            print(\"[INFO] create RNN component successfully, rnn : %s .\" % rnn)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "    def create_classifier(self, n_label=11):\n",
    "        try:\n",
    "            if self.backend in ['vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn'] :\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Linear(1024, 1024),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(1024, 11),\n",
    "                    nn.Softmax(),\n",
    "                )\n",
    "            else :\n",
    "                raise ValueError(\"[ERROR] Unexpected backend name pass through previous check then into create_outLayer() .\")\n",
    "        \n",
    "            print(\"[INFO] create classifier successfully.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                \n",
    "    def fix_features(self): # fix features weights\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def load_Problem2_pretrain(self, path='./storage/'):\n",
    "        if self.device == 'cuda':\n",
    "            self.RNN.load_state_dict(torch.load(os.path.join(path, 'MFrnnVGG_RNN.pkl')))\n",
    "            self.classifier.load_state_dict(torch.load(os.path.join(path, 'MFrnnVGG_classifier.pkl')))\n",
    "        else :\n",
    "            self.RNN.load_state_dict(torch.load(os.path.join(path, 'MFrnnVGG_RNN.pkl'), map_location=lambda storage, loc: storage))\n",
    "            self.classifier.load_state_dict(torch.load(os.path.join(path, 'MFrnnVGG_classifier.pkl'), map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        print(\"[INFO] load Problem2 pretrained weight successfully.\")\n",
    "\n",
    "\n",
    "model = FVrnnVGG(backend='vgg13_bn')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "modell = models.vgg13()\n",
    "modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU is useless when batch size = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch (0/200), cost: 93 sec | loss : 2.118162 | acc : 42.99%\n",
      "[INFO] epoch (1/200), cost: 93 sec | loss : 2.055091 | acc : 49.38%\n",
      "[INFO] epoch (2/200), cost: 93 sec | loss : 2.040490 | acc : 50.88%\n",
      "[INFO] epoch (3/200), cost: 93 sec | loss : 2.028677 | acc : 51.90%\n",
      "[INFO] epoch (4/200), cost: 93 sec | loss : 2.020675 | acc : 52.91%\n",
      "[INFO] epoch (5/200), cost: 93 sec | loss : 2.014509 | acc : 53.42%\n",
      "[INFO] epoch (6/200), cost: 93 sec | loss : 2.009074 | acc : 54.06%\n",
      "[INFO] epoch (7/200), cost: 93 sec | loss : 2.004512 | acc : 54.28%\n",
      "[INFO] epoch (8/200), cost: 93 sec | loss : 2.001054 | acc : 54.55%\n",
      "[INFO] epoch (9/200), cost: 93 sec | loss : 1.995099 | acc : 55.52%\n",
      "[INFO] epoch (10/200), cost: 93 sec | loss : 1.989475 | acc : 55.90%\n",
      "[INFO] epoch (11/200), cost: 93 sec | loss : 1.986925 | acc : 56.12%\n",
      "[INFO] epoch (12/200), cost: 93 sec | loss : 1.983295 | acc : 56.62%\n",
      "[INFO] epoch (13/200), cost: 93 sec | loss : 1.979444 | acc : 56.81%\n",
      "[INFO] epoch (14/200), cost: 93 sec | loss : 1.977201 | acc : 57.23%\n",
      "[INFO] epoch (15/200), cost: 93 sec | loss : 1.975321 | acc : 57.28%\n",
      "[INFO] epoch (16/200), cost: 93 sec | loss : 1.972399 | acc : 57.65%\n",
      "[INFO] epoch (17/200), cost: 93 sec | loss : 1.971244 | acc : 57.94%\n",
      "[INFO] epoch (18/200), cost: 93 sec | loss : 1.969824 | acc : 57.90%\n",
      "[INFO] epoch (19/200), cost: 93 sec | loss : 1.967238 | acc : 58.29%\n",
      "[INFO] epoch (20/200), cost: 93 sec | loss : 1.965121 | acc : 58.45%\n",
      "[INFO] epoch (21/200), cost: 93 sec | loss : 1.963547 | acc : 58.45%\n",
      "[INFO] epoch (22/200), cost: 93 sec | loss : 1.961968 | acc : 58.77%\n",
      "[INFO] epoch (23/200), cost: 93 sec | loss : 1.962091 | acc : 58.52%\n",
      "[INFO] epoch (24/200), cost: 93 sec | loss : 1.959969 | acc : 58.86%\n",
      "[INFO] epoch (25/200), cost: 93 sec | loss : 1.958904 | acc : 58.97%\n",
      "[INFO] epoch (26/200), cost: 93 sec | loss : 1.956253 | acc : 59.38%\n",
      "[INFO] epoch (27/200), cost: 93 sec | loss : 1.955855 | acc : 59.33%\n",
      "[INFO] epoch (28/200), cost: 93 sec | loss : 1.955915 | acc : 59.29%\n",
      "[INFO] epoch (29/200), cost: 93 sec | loss : 1.953421 | acc : 59.54%\n",
      "[INFO] epoch (30/200), cost: 93 sec | loss : 1.953822 | acc : 59.55%\n",
      "[INFO] epoch (31/200), cost: 93 sec | loss : 1.951462 | acc : 59.77%\n",
      "[INFO] epoch (32/200), cost: 93 sec | loss : 1.951256 | acc : 59.68%\n",
      "[INFO] epoch (33/200), cost: 93 sec | loss : 1.949925 | acc : 59.70%\n",
      "[INFO] epoch (34/200), cost: 93 sec | loss : 1.947913 | acc : 60.13%\n",
      "[INFO] epoch (35/200), cost: 93 sec | loss : 1.946940 | acc : 60.13%\n",
      "[INFO] epoch (36/200), cost: 93 sec | loss : 1.946636 | acc : 60.16%\n",
      "[INFO] epoch (37/200), cost: 93 sec | loss : 1.945449 | acc : 60.32%\n",
      "[INFO] epoch (38/200), cost: 93 sec | loss : 1.944260 | acc : 60.54%\n",
      "[INFO] epoch (39/200), cost: 93 sec | loss : 1.944400 | acc : 60.33%\n",
      "[INFO] epoch (40/200), cost: 93 sec | loss : 1.943049 | acc : 60.57%\n",
      "[INFO] epoch (41/200), cost: 93 sec | loss : 1.942144 | acc : 60.78%\n",
      "[INFO] epoch (42/200), cost: 93 sec | loss : 1.941035 | acc : 60.86%\n",
      "[INFO] epoch (43/200), cost: 93 sec | loss : 1.939776 | acc : 60.84%\n",
      "[INFO] epoch (44/200), cost: 93 sec | loss : 1.938336 | acc : 61.01%\n",
      "[INFO] epoch (45/200), cost: 93 sec | loss : 1.936501 | acc : 61.36%\n",
      "[INFO] epoch (46/200), cost: 93 sec | loss : 1.935764 | acc : 61.23%\n",
      "[INFO] epoch (47/200), cost: 93 sec | loss : 1.936321 | acc : 61.30%\n",
      "[INFO] epoch (48/200), cost: 93 sec | loss : 1.934520 | acc : 61.54%\n",
      "[INFO] epoch (49/200), cost: 93 sec | loss : 1.933557 | acc : 61.55%\n",
      "[INFO] epoch (50/200), cost: 93 sec | loss : 1.932222 | acc : 61.67%\n",
      "[INFO] epoch (51/200), cost: 93 sec | loss : 1.931597 | acc : 61.88%\n",
      "[INFO] epoch (52/200), cost: 93 sec | loss : 1.931661 | acc : 61.77%\n",
      "[INFO] epoch (53/200), cost: 93 sec | loss : 1.929496 | acc : 61.97%\n",
      "[INFO] epoch (54/200), cost: 93 sec | loss : 1.928837 | acc : 62.16%\n",
      "[INFO] epoch (55/200), cost: 93 sec | loss : 1.928614 | acc : 62.00%\n",
      "[INFO] epoch (56/200), cost: 93 sec | loss : 1.928299 | acc : 62.12%\n",
      "[INFO] epoch (57/200), cost: 93 sec | loss : 1.926448 | acc : 62.35%\n",
      "[INFO] epoch (58/200), cost: 93 sec | loss : 1.926119 | acc : 62.38%\n",
      "[INFO] epoch (59/200), cost: 93 sec | loss : 1.924255 | acc : 62.64%\n",
      "[INFO] epoch (60/200), cost: 93 sec | loss : 1.923210 | acc : 62.72%\n",
      "[INFO] epoch (61/200), cost: 93 sec | loss : 1.923389 | acc : 62.72%\n",
      "[INFO] epoch (62/200), cost: 93 sec | loss : 1.922595 | acc : 62.74%\n",
      "[INFO] epoch (63/200), cost: 93 sec | loss : 1.920431 | acc : 62.93%\n",
      "[INFO] epoch (64/200), cost: 93 sec | loss : 1.920222 | acc : 62.94%\n",
      "[INFO] epoch (65/200), cost: 93 sec | loss : 1.920504 | acc : 62.99%\n",
      "[INFO] epoch (66/200), cost: 93 sec | loss : 1.919462 | acc : 63.06%\n",
      "[INFO] epoch (67/200), cost: 93 sec | loss : 1.917080 | acc : 63.12%\n",
      "[INFO] epoch (68/200), cost: 93 sec | loss : 1.918577 | acc : 63.19%\n",
      "[INFO] epoch (69/200), cost: 93 sec | loss : 1.915524 | acc : 63.48%\n",
      "[INFO] epoch (70/200), cost: 93 sec | loss : 1.915460 | acc : 63.33%\n",
      "[INFO] epoch (71/200), cost: 93 sec | loss : 1.913872 | acc : 63.59%\n",
      "[INFO] epoch (72/200), cost: 93 sec | loss : 1.914023 | acc : 63.61%\n",
      "[INFO] epoch (73/200), cost: 93 sec | loss : 1.913259 | acc : 63.74%\n",
      "[INFO] epoch (74/200), cost: 93 sec | loss : 1.912733 | acc : 63.68%\n",
      "[INFO] epoch (75/200), cost: 93 sec | loss : 1.912468 | acc : 63.81%\n",
      "[INFO] epoch (76/200), cost: 93 sec | loss : 1.910725 | acc : 64.00%\n",
      "[INFO] epoch (77/200), cost: 93 sec | loss : 1.909750 | acc : 63.96%\n",
      "[INFO] epoch (78/200), cost: 93 sec | loss : 1.910462 | acc : 63.94%\n",
      "[INFO] epoch (79/200), cost: 93 sec | loss : 1.909992 | acc : 63.87%\n",
      "[INFO] epoch (80/200), cost: 93 sec | loss : 1.908563 | acc : 64.14%\n",
      "[INFO] epoch (81/200), cost: 93 sec | loss : 1.908574 | acc : 64.20%\n",
      "[INFO] epoch (82/200), cost: 93 sec | loss : 1.907401 | acc : 64.33%\n",
      "[INFO] epoch (83/200), cost: 93 sec | loss : 1.906168 | acc : 64.36%\n",
      "[INFO] epoch (84/200), cost: 93 sec | loss : 1.905534 | acc : 64.43%\n",
      "[INFO] epoch (85/200), cost: 93 sec | loss : 1.905808 | acc : 64.36%\n",
      "[INFO] epoch (86/200), cost: 93 sec | loss : 1.904654 | acc : 64.51%\n",
      "[INFO] epoch (87/200), cost: 93 sec | loss : 1.904088 | acc : 64.64%\n",
      "[INFO] epoch (88/200), cost: 93 sec | loss : 1.903717 | acc : 64.71%\n",
      "[INFO] epoch (89/200), cost: 93 sec | loss : 1.903344 | acc : 64.68%\n",
      "[INFO] epoch (90/200), cost: 93 sec | loss : 1.902260 | acc : 64.61%\n",
      "[INFO] epoch (91/200), cost: 93 sec | loss : 1.902304 | acc : 64.75%\n",
      "[INFO] epoch (92/200), cost: 93 sec | loss : 1.900853 | acc : 64.93%\n",
      "[INFO] epoch (93/200), cost: 93 sec | loss : 1.900416 | acc : 65.01%\n",
      "[INFO] epoch (94/200), cost: 93 sec | loss : 1.900518 | acc : 64.94%\n",
      "[INFO] epoch (95/200), cost: 93 sec | loss : 1.899365 | acc : 65.12%\n",
      "[INFO] epoch (96/200), cost: 93 sec | loss : 1.897415 | acc : 65.20%\n",
      "[INFO] epoch (97/200), cost: 93 sec | loss : 1.898308 | acc : 65.25%\n",
      "[INFO] epoch (98/200), cost: 93 sec | loss : 1.897485 | acc : 65.33%\n",
      "[INFO] epoch (99/200), cost: 93 sec | loss : 1.897074 | acc : 65.28%\n",
      "[INFO] epoch (100/200), cost: 93 sec | loss : 1.895879 | acc : 65.36%\n",
      "[INFO] epoch (101/200), cost: 93 sec | loss : 1.895647 | acc : 65.30%\n",
      "[INFO] epoch (102/200), cost: 93 sec | loss : 1.894998 | acc : 65.57%\n",
      "[INFO] epoch (103/200), cost: 93 sec | loss : 1.894773 | acc : 65.46%\n",
      "[INFO] epoch (104/200), cost: 93 sec | loss : 1.893898 | acc : 65.58%\n",
      "[INFO] epoch (105/200), cost: 93 sec | loss : 1.894264 | acc : 65.54%\n",
      "[INFO] epoch (106/200), cost: 93 sec | loss : 1.893349 | acc : 65.61%\n",
      "[INFO] epoch (107/200), cost: 93 sec | loss : 1.892184 | acc : 65.77%\n",
      "[INFO] epoch (108/200), cost: 93 sec | loss : 1.892320 | acc : 65.70%\n",
      "[INFO] epoch (109/200), cost: 93 sec | loss : 1.891646 | acc : 65.75%\n",
      "[INFO] epoch (110/200), cost: 93 sec | loss : 1.891835 | acc : 65.72%\n",
      "[INFO] epoch (111/200), cost: 93 sec | loss : 1.890193 | acc : 65.99%\n",
      "[INFO] epoch (112/200), cost: 93 sec | loss : 1.889227 | acc : 66.04%\n",
      "[INFO] epoch (113/200), cost: 93 sec | loss : 1.889258 | acc : 66.06%\n",
      "[INFO] epoch (114/200), cost: 93 sec | loss : 1.888107 | acc : 66.19%\n",
      "[INFO] epoch (115/200), cost: 93 sec | loss : 1.889199 | acc : 66.10%\n",
      "[INFO] epoch (116/200), cost: 93 sec | loss : 1.887642 | acc : 66.13%\n",
      "[INFO] epoch (117/200), cost: 93 sec | loss : 1.887046 | acc : 66.26%\n",
      "[INFO] epoch (118/200), cost: 93 sec | loss : 1.886726 | acc : 66.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch (119/200), cost: 93 sec | loss : 1.885164 | acc : 66.46%\n",
      "[INFO] epoch (120/200), cost: 93 sec | loss : 1.885074 | acc : 66.48%\n",
      "[INFO] epoch (121/200), cost: 93 sec | loss : 1.884904 | acc : 66.43%\n",
      "[INFO] epoch (122/200), cost: 93 sec | loss : 1.883594 | acc : 66.64%\n",
      "[INFO] epoch (123/200), cost: 93 sec | loss : 1.884718 | acc : 66.49%\n",
      "[INFO] epoch (124/200), cost: 93 sec | loss : 1.882836 | acc : 66.62%\n",
      "[INFO] epoch (125/200), cost: 93 sec | loss : 1.882990 | acc : 66.81%\n",
      "[INFO] epoch (126/200), cost: 93 sec | loss : 1.880938 | acc : 66.80%\n",
      "[INFO] epoch (127/200), cost: 93 sec | loss : 1.881171 | acc : 66.81%\n",
      "[INFO] epoch (128/200), cost: 93 sec | loss : 1.881026 | acc : 66.90%\n",
      "[INFO] epoch (129/200), cost: 93 sec | loss : 1.880094 | acc : 66.90%\n",
      "[INFO] epoch (130/200), cost: 93 sec | loss : 1.879983 | acc : 66.84%\n",
      "[INFO] epoch (131/200), cost: 93 sec | loss : 1.879718 | acc : 67.13%\n",
      "[INFO] epoch (132/200), cost: 93 sec | loss : 1.878240 | acc : 67.09%\n",
      "[INFO] epoch (133/200), cost: 93 sec | loss : 1.877309 | acc : 67.26%\n",
      "[INFO] epoch (134/200), cost: 93 sec | loss : 1.877508 | acc : 67.10%\n",
      "[INFO] epoch (135/200), cost: 93 sec | loss : 1.877080 | acc : 67.26%\n",
      "[INFO] epoch (136/200), cost: 93 sec | loss : 1.876233 | acc : 67.20%\n",
      "[INFO] epoch (137/200), cost: 93 sec | loss : 1.876404 | acc : 67.38%\n",
      "[INFO] epoch (138/200), cost: 93 sec | loss : 1.876273 | acc : 67.26%\n",
      "[INFO] epoch (139/200), cost: 93 sec | loss : 1.876512 | acc : 67.28%\n",
      "[INFO] epoch (140/200), cost: 93 sec | loss : 1.874556 | acc : 67.43%\n",
      "[INFO] epoch (141/200), cost: 93 sec | loss : 1.874174 | acc : 67.57%\n",
      "[INFO] epoch (142/200), cost: 93 sec | loss : 1.874321 | acc : 67.39%\n",
      "[INFO] epoch (143/200), cost: 93 sec | loss : 1.873284 | acc : 67.55%\n",
      "[INFO] epoch (144/200), cost: 93 sec | loss : 1.873372 | acc : 67.59%\n",
      "[INFO] epoch (145/200), cost: 93 sec | loss : 1.872548 | acc : 67.77%\n",
      "[INFO] epoch (146/200), cost: 93 sec | loss : 1.872383 | acc : 67.62%\n",
      "[INFO] epoch (147/200), cost: 93 sec | loss : 1.871260 | acc : 67.78%\n",
      "[INFO] epoch (148/200), cost: 93 sec | loss : 1.870721 | acc : 67.83%\n",
      "[INFO] epoch (149/200), cost: 93 sec | loss : 1.871231 | acc : 67.78%\n",
      "[INFO] epoch (150/200), cost: 93 sec | loss : 1.870277 | acc : 67.87%\n",
      "[INFO] epoch (151/200), cost: 93 sec | loss : 1.870005 | acc : 67.86%\n",
      "[INFO] epoch (152/200), cost: 93 sec | loss : 1.869202 | acc : 68.03%\n",
      "[INFO] epoch (153/200), cost: 93 sec | loss : 1.868914 | acc : 68.13%\n",
      "[INFO] epoch (154/200), cost: 93 sec | loss : 1.868924 | acc : 68.03%\n",
      "[INFO] epoch (155/200), cost: 93 sec | loss : 1.869409 | acc : 67.88%\n",
      "[INFO] epoch (156/200), cost: 93 sec | loss : 1.868336 | acc : 68.04%\n",
      "[INFO] epoch (157/200), cost: 93 sec | loss : 1.866683 | acc : 68.17%\n",
      "[INFO] epoch (158/200), cost: 93 sec | loss : 1.866661 | acc : 68.17%\n",
      "[INFO] epoch (159/200), cost: 93 sec | loss : 1.867528 | acc : 68.14%\n",
      "[INFO] epoch (160/200), cost: 93 sec | loss : 1.866019 | acc : 68.35%\n",
      "[INFO] epoch (161/200), cost: 93 sec | loss : 1.865884 | acc : 68.33%\n",
      "[INFO] epoch (162/200), cost: 93 sec | loss : 1.866464 | acc : 68.26%\n",
      "[INFO] epoch (163/200), cost: 93 sec | loss : 1.864545 | acc : 68.48%\n",
      "[INFO] epoch (164/200), cost: 93 sec | loss : 1.864525 | acc : 68.49%\n",
      "[INFO] epoch (165/200), cost: 93 sec | loss : 1.863834 | acc : 68.49%\n",
      "[INFO] epoch (166/200), cost: 93 sec | loss : 1.864597 | acc : 68.52%\n",
      "[INFO] epoch (167/200), cost: 93 sec | loss : 1.864025 | acc : 68.45%\n",
      "[INFO] epoch (168/200), cost: 93 sec | loss : 1.862735 | acc : 68.64%\n",
      "[INFO] epoch (169/200), cost: 93 sec | loss : 1.863349 | acc : 68.64%\n",
      "[INFO] epoch (170/200), cost: 93 sec | loss : 1.862601 | acc : 68.61%\n",
      "[INFO] epoch (171/200), cost: 93 sec | loss : 1.862549 | acc : 68.65%\n",
      "[INFO] epoch (172/200), cost: 93 sec | loss : 1.861662 | acc : 68.74%\n",
      "[INFO] epoch (173/200), cost: 93 sec | loss : 1.861390 | acc : 68.78%\n",
      "[INFO] epoch (174/200), cost: 93 sec | loss : 1.860574 | acc : 68.88%\n",
      "[INFO] epoch (175/200), cost: 93 sec | loss : 1.860288 | acc : 68.83%\n",
      "[INFO] epoch (176/200), cost: 93 sec | loss : 1.860370 | acc : 68.94%\n",
      "[INFO] epoch (177/200), cost: 93 sec | loss : 1.859876 | acc : 68.94%\n",
      "[INFO] epoch (178/200), cost: 93 sec | loss : 1.859011 | acc : 68.96%\n",
      "[INFO] epoch (179/200), cost: 93 sec | loss : 1.859377 | acc : 69.07%\n",
      "[INFO] epoch (180/200), cost: 93 sec | loss : 1.859058 | acc : 69.04%\n",
      "[INFO] epoch (181/200), cost: 93 sec | loss : 1.857685 | acc : 69.09%\n",
      "[INFO] epoch (182/200), cost: 93 sec | loss : 1.857546 | acc : 69.17%\n",
      "[INFO] epoch (183/200), cost: 93 sec | loss : 1.856866 | acc : 69.20%\n",
      "[INFO] epoch (184/200), cost: 93 sec | loss : 1.856816 | acc : 69.22%\n",
      "[INFO] epoch (185/200), cost: 93 sec | loss : 1.856107 | acc : 69.16%\n",
      "[INFO] epoch (186/200), cost: 93 sec | loss : 1.855585 | acc : 69.32%\n",
      "[INFO] epoch (187/200), cost: 93 sec | loss : 1.855477 | acc : 69.32%\n",
      "[INFO] epoch (188/200), cost: 93 sec | loss : 1.855179 | acc : 69.41%\n",
      "[INFO] epoch (189/200), cost: 93 sec | loss : 1.854918 | acc : 69.38%\n",
      "[INFO] epoch (190/200), cost: 93 sec | loss : 1.854294 | acc : 69.57%\n",
      "[INFO] epoch (191/200), cost: 93 sec | loss : 1.853833 | acc : 69.54%\n",
      "[INFO] epoch (192/200), cost: 93 sec | loss : 1.854220 | acc : 69.48%\n",
      "[INFO] epoch (193/200), cost: 93 sec | loss : 1.853082 | acc : 69.64%\n",
      "[INFO] epoch (194/200), cost: 93 sec | loss : 1.852757 | acc : 69.74%\n",
      "[INFO] epoch (195/200), cost: 93 sec | loss : 1.852959 | acc : 69.58%\n",
      "[INFO] epoch (196/200), cost: 93 sec | loss : 1.851814 | acc : 69.81%\n",
      "[INFO] epoch (197/200), cost: 93 sec | loss : 1.851295 | acc : 69.80%\n",
      "[INFO] epoch (198/200), cost: 93 sec | loss : 1.851675 | acc : 69.81%\n",
      "[INFO] epoch (199/200), cost: 93 sec | loss : 1.850952 | acc : 69.86%\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor, LongTensor\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "lr=1e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(list(model.RNN.parameters()), lr=lr)\n",
    "\n",
    "model.to(device)\n",
    "# model.h0.to(device)\n",
    "\n",
    "\n",
    "#################### history ####################\n",
    "history = {}\n",
    "history['loss'] = []\n",
    "history['acc'] = []\n",
    "\n",
    "history['err'] = {}\n",
    "history['err']['epoch'] = []\n",
    "history['err']['err_idx'] = []\n",
    "history['err']['err_msg'] = []\n",
    "#################################################\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time()\n",
    "    total_loss = 0.\n",
    "    acc = 0.\n",
    "    cnt = 0\n",
    "    \n",
    "    for idx, (X, Y) in enumerate(zip(train_x, train_y)):\n",
    "        X = np.transpose(X, (0,3,1,2)) # transpose for torch input : shape (f, 240, 320, 3) --> (f, 3, 240, 320)\n",
    "        \n",
    "        \n",
    "#         try:\n",
    "            # because of lack of calculate resources, i devide each videos to 10 parts with 20 frames foreach.\n",
    "        interval = 20\n",
    "        h = model.h0\n",
    "        for idx in range(0, X.shape[0])[::interval]:\n",
    "            x = X[idx:idx+interval]\n",
    "            y = Y[idx:idx+interval]\n",
    "\n",
    "            x = Variable(FloatTensor(x)).to(device)\n",
    "            y = Variable(LongTensor(y)).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred, h = model(x, h)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            acc += (sum(pred.argmax(dim=1) == y).item() / interval)\n",
    "            total_loss += loss.item()\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "#         except Exception as e:\n",
    "#             history['err']['epoch'].append(epoch)\n",
    "#             history['err']['err_idx'].append(idx)\n",
    "#             history['err']['err_msg'].append(str(e))\n",
    "        \n",
    "        \n",
    "    history['loss'].append(total_loss/cnt)\n",
    "    history['acc'].append((100*acc)/cnt)\n",
    "    print('[INFO] epoch (%d/%d), cost: %d sec | loss : %.6f | acc : %.2f%%' % (epoch, epochs, (time()-start_time), (total_loss/cnt), (100*acc/cnt)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.classifier.state_dict(), './storage/FVrnnVGG_classifier.pkl')\n",
    "torch.save(model.RNN.state_dict(), './storage/FVrnnVGG_RNN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('./storage/history_p2_FVrnnVGG_vgg13bn_epoch200', 'wb') as handle:\n",
    "    pickle.dump(history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid and report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 1012 ... finish, local accuracy:16.50%\n",
      "len: 1085 ... finish, local accuracy:16.07%\n",
      "len: 2471 ... finish, local accuracy:11.97%\n",
      "len: 982 ... finish, local accuracy:12.68%\n",
      "len: 889 ... finish, local accuracy:13.36%\n",
      "len: 948 ... finish, local accuracy:13.01%\n",
      "len: 1551 ... finish, local accuracy:13.11%\n",
      "[INFO] finish, correct:1172 and total:8938, accuracy is 13.11%\n"
     ]
    }
   ],
   "source": [
    "task = \"valid\"\n",
    "\n",
    "names = []\n",
    "label_y = []\n",
    "pred_y = []\n",
    "correct = 0 \n",
    "cnt = 0\n",
    "\n",
    "for label_path in glob('./hw4_data/FullLengthVideos/labels/'+task+'/*') :\n",
    "    name = label_path.split('/')[-1].replace('.txt','')\n",
    "#     print('[INFO] load images and label for video :', name, '...', end='')\n",
    "    pred = []\n",
    "    \n",
    "    \n",
    "    # load labels\n",
    "    labels = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for label in file:\n",
    "            labels.append(int(label.replace('\\n','')))\n",
    "    \n",
    "    with open(os.path.join('./output/', name+'.txt'), 'r') as file:\n",
    "        for line in file:\n",
    "            pred.append(int(line.replace('\\n','')))\n",
    "   \n",
    "    # to numpy\n",
    "    labels = np.array(labels)\n",
    "    pred = np.array(pred)\n",
    "    \n",
    "    # agg\n",
    "    if not len(pred) == len(labels):\n",
    "        raise ValueError('[ERROR] Mismatch between pred and label, pred:%d  , label:%d' % (len(pred, len(labels))))\n",
    "    cnt += len(pred)\n",
    "    correct += sum(labels == pred)\n",
    "    localAcc = 100 * correct/cnt\n",
    "    \n",
    "    # finally put into train_x and train_y\n",
    "    names.append(name)\n",
    "    label_y.append(labels)\n",
    "    pred_y.append(pred)\n",
    "    print('len: %d ... finish, local accuracy:%.2f%%' % (len(labels), localAcc)) \n",
    "    \n",
    "    \n",
    "print('[INFO] finish, correct:%d and total:%d, accuracy is %.2f%%' % (correct, cnt, 100*correct/cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visual result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2, 3, 4, 5, 6, 8}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose first video (acc:16.50%)\n",
    "idx = 0\n",
    "name = names[idx]\n",
    "label = label_y[0]\n",
    "pred = pred_y[0]\n",
    "\n",
    "idx = list(np.arange(len(label)))\n",
    "set(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
